{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd320f2c",
   "metadata": {},
   "source": [
    "<h1>Parking Spaces</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf2f04",
   "metadata": {},
   "source": [
    "<h2>import all libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1191845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "\n",
    "from skimage.transform import resize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd0a21",
   "metadata": {},
   "source": [
    "<h2>helper functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c2a849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "EMPTY = True\n",
    "NOT_EMPTY = False\n",
    "\n",
    "# Load the pre-trained machine learning model\n",
    "MODEL = pickle.load(open(\"model.p\", \"rb\"))\n",
    "\n",
    "# Function to determine if a parking spot is empty or not based on a given image\n",
    "def empty_or_not(spot_bgr):\n",
    "    # Flatten and resize the image data\n",
    "    flat_data = []\n",
    "    img_resized = resize(spot_bgr, (15, 15, 3))\n",
    "    flat_data.append(img_resized.flatten())\n",
    "    flat_data = np.array(flat_data)\n",
    "\n",
    "    # Predict using the pre-trained model\n",
    "    y_output = MODEL.predict(flat_data)\n",
    "\n",
    "    # Return whether the parking spot is empty or not based on the model prediction\n",
    "    if y_output == 0:\n",
    "        return EMPTY\n",
    "    else:\n",
    "        return NOT_EMPTY\n",
    "\n",
    "# Function to extract bounding boxes of parking spots from connected components\n",
    "def get_parking_spots_bboxes(connected_components):\n",
    "    (totalLabels, label_ids, values, centroid) = connected_components\n",
    "\n",
    "    # Initialize a list to store bounding boxes of parking spots\n",
    "    slots = []\n",
    "    coef = 1\n",
    "\n",
    "    # Loop through connected components and extract bounding box coordinates\n",
    "    for i in range(1, totalLabels):\n",
    "        x1 = int(values[i, cv2.CC_STAT_LEFT] * coef)\n",
    "        y1 = int(values[i, cv2.CC_STAT_TOP] * coef)\n",
    "        w = int(values[i, cv2.CC_STAT_WIDTH] * coef)\n",
    "        h = int(values[i, cv2.CC_STAT_HEIGHT] * coef)\n",
    "\n",
    "        # Append bounding box coordinates to the list\n",
    "        slots.append([x1, y1, w, h])\n",
    "\n",
    "    return slots\n",
    "\n",
    "# Function to calculate the absolute difference between the mean intensities of two images\n",
    "def calc_diff(im1, im2):\n",
    "    return np.abs(np.mean(im1) - np.mean(im2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f69b4c",
   "metadata": {},
   "source": [
    "<h2>Import the video</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b486dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the video file\n",
    "video_path = './parking_1920_1080_loop.mp4'\n",
    "\n",
    "# Open a video capture object using OpenCV\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e835349",
   "metadata": {},
   "source": [
    "<h2>find bounding boxes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b1faec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[533, 98, 65, 35]\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the mask image\n",
    "mask_path = './mask_1920_1080.png'\n",
    "\n",
    "# Read the mask image in grayscale\n",
    "mask = cv2.imread(mask_path, 0)\n",
    "\n",
    "# Use connected components to label and extract statistics about connected regions in the mask\n",
    "connected_components = cv2.connectedComponentsWithStats(mask, 4, cv2.CV_32S)\n",
    "\n",
    "# Get bounding boxes (x, y, width, height) for parking spots from the connected components\n",
    "spots = get_parking_spots_bboxes(connected_components)\n",
    "\n",
    "# Print the bounding box coordinates of the first parking spot\n",
    "print(spots[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d4ab8",
   "metadata": {},
   "source": [
    "<h2>Play and Manipulate Video</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bb39464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store parking spot statuses and differences between frames\n",
    "spots_status = [None for j in spots]\n",
    "diffs = [None for j in spots]\n",
    "\n",
    "# Initialize variables for tracking the previous frame\n",
    "previous_frame = None\n",
    "\n",
    "# Initialize frame number, video capture, and processing step\n",
    "frame_nmb = 0\n",
    "ret = True\n",
    "step = 30\n",
    "\n",
    "# Loop through video frames\n",
    "while ret:\n",
    "    # Read the next frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Process every 30 frames and calculate differences\n",
    "    if frame_nmb % step == 0 and previous_frame is not None:\n",
    "        for spot_index, spot in enumerate(spots):\n",
    "            x1, y1, w, h = spot\n",
    "\n",
    "            # Crop the current and previous frames to the parking spot\n",
    "            spot_crop = frame[y1:y1 + h, x1:x1 + w, :]\n",
    "            previous_spot_crop = previous_frame[y1:y1 + h, x1:x1 + w, :]\n",
    "\n",
    "            # Calculate the difference between the current and previous frames\n",
    "            diffs[spot_index] = calc_diff(spot_crop, previous_spot_crop)\n",
    "\n",
    "    # Process every 30 frames\n",
    "    if frame_nmb % step == 0:\n",
    "        # Determine the order of spots to check based on differences\n",
    "        if previous_frame is None:\n",
    "            arr_ = range(len(spots))\n",
    "        else:\n",
    "            # Sort spots based on differences and filter by threshold\n",
    "            arr_ = [j for j in np.argsort(diffs) if diffs[j] / np.amax(diffs) > 0.4][::-1]\n",
    "\n",
    "        # Loop through the sorted spots\n",
    "        for spot_index in arr_:\n",
    "            spot = spots[spot_index]\n",
    "            x1, y1, w, h = spot\n",
    "\n",
    "            # Crop the frame to the parking spot\n",
    "            spot_crop = frame[y1:y1 + h, x1:x1 + w, :]\n",
    "\n",
    "            # Use the model to predict whether the parking spot is empty or not\n",
    "            spot_status = empty_or_not(spot_crop)\n",
    "            spots_status[spot_index] = spot_status\n",
    "\n",
    "    # Process every 30 frames\n",
    "    if frame_nmb % step == 0:\n",
    "        # Update the previous frame\n",
    "        previous_frame = frame.copy()\n",
    "\n",
    "    # Draw rectangles and display information on the frame\n",
    "    for spot_index, spot in enumerate(spots):\n",
    "        spot_status = spots_status[spot_index]\n",
    "        x1, y1, w, h = spots[spot_index]\n",
    "\n",
    "        # Draw rectangles based on spot status\n",
    "        if spot_status:\n",
    "            cv2.rectangle(frame, (x1, y1), (x1 + w, y1 + h), (0, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.rectangle(frame, (x1, y1), (x1 + w, y1 + h), (0, 0, 255), 2)\n",
    "\n",
    "    # Draw a rectangle and display information about available spots\n",
    "    cv2.rectangle(frame, (80, 20), (550, 80), (0, 0, 0), -1)\n",
    "    cv2.putText(frame, 'Available Spots: {} / {}'.format(str(sum(spots_status)), str(len(spots_status))),\n",
    "                (100, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 22), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.namedWindow('window', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Increment frame number\n",
    "    frame_nmb += 1\n",
    "\n",
    "# Release the video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521e78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
